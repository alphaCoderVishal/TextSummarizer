{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef6bb41",
   "metadata": {},
   "source": [
    "# Abstractive:\n",
    "generate new text that captures the most relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c8e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431b33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4f1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2d29e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8244b2381dce4e0083bb63923b2106c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b0978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (C:/Users/vishal567795/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    }
   ],
   "source": [
    "# so we are using the billsum data set here\n",
    "from datasets import load_dataset\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f8e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6a14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into a train and test set with the train_test_split method:\n",
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed685c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nSection 23102 of the\\nRevenue and Taxation Code\\nis amended to read:\\n23102.\\nAny corporation or limited liability company holding or organized to hold stock or bonds of any other corporation or corporations, and not trading in stock or bonds or other securities held, and engaging in no activities other than the receipt and disbursement of dividends from stock or interest from bonds, and no activities other than those exempted under subdivision (c) of Section 191 of the Corporations Code, is not a corporation or limited liability company doing business in this State for the purposes of this chapter or Chapter 10.6.\\nSECTION 1.\\nSection 17941 of the Revenue and Taxation Code is amended to read:\\n17941.\\n(a) For each taxable year beginning on or after January 1, 1997, a limited liability company doing business in this\\nstate (as\\nstate, as\\ndefined in Section\\n23101)\\n23101,\\nshall pay annually to this state a tax for the privilege of doing business in this state in an amount equal to the applicable amount specified in\\nparagraph (1) of\\nsubdivision (d) of Section 23153 for the taxable year.\\n(b) (1) In addition to any limited liability company that is doing business in this state and is therefore subject to the tax imposed by subdivision (a), for each taxable year beginning on or after January 1, 1997, a limited liability company shall pay annually the tax prescribed in subdivision (a) if articles of organization have been accepted, or a certificate of registration has been issued, by the office of the Secretary of State. The tax shall be paid for each taxable year, or part thereof, until a certificate of cancellation of registration or of articles of organization is filed on behalf of the limited liability company with the office of the Secretary of State.\\n(2) If a taxpayer files a return with the Franchise Tax Board that is designated as its final return, the Franchise Tax Board shall notify the taxpayer that the annual tax shall continue to be due annually until a certificate of dissolution is filed with the Secretary of State pursuant to Section 17707.08 of the Corporations Code or a certificate of cancellation is filed with the Secretary of State pursuant to Section 17708.06 of the Corporations Code.\\n(c) The tax assessed under this section shall be due and payable on or before the 15th day of the fourth month of the taxable year.\\n(d)\\nFor\\n(1)\\nExcept as provided in paragraph (2), for\\npurposes of this section,\\na\\n“limited liability company” means an\\norganization, other than a limited liability company that is exempt from the tax and fees imposed under this chapter pursuant to Section 23701h or Section 23701x,\\norganization\\nthat is formed by one or more persons under the law of this state, any other country, or any other state, as a “limited liability company” and that is not taxable as a corporation for California tax purposes.\\n(2) Notwithstanding subdivisions (a) and (b), a limited liability company is not subject to the tax imposed under this section if it is either of the following:\\n(A) The limited liability company is exempt from the tax and fees imposed under this chapter pursuant to Section 23701h or 23701x.\\n(B) (i) The limited liability company is a qualified investment partnership.\\n(ii) For purposes of this subparagraph, a qualified investment partnership means a limited liability company that meets all of the following requirements:\\n(I) It is classified as a partnership for California income tax purposes.\\n(II) No less than 90 percent of the costs of its total assets consist of qualifying investment securities, deposits at banks or other financial institutions, interest or investments in a partnership, or office space and equipment reasonably necessary to carry on its activities as a qualified investment partnership.\\n(III) No less than 90 percent of its gross income consists of interest, dividends, and gains from the sale or exchange of qualifying investment securities or investments in a partnership.\\n(iii) For purposes of this subparagraph, “qualifying investment securities” has the same meaning as that term is described in subparagraph (A) of paragraph (3) of subdivision (c) of Section 17955.\\n(iv) Notwithstanding Section 18633.5, the following rules shall apply with respect to the filing requirements of a qualified investment partnership.\\n(I) A qualified investment partnership required to file a federal return pursuant to Section 6031 of the Internal Revenue Code, relating to return of partnership income, shall file a partnership return pursuant to Section 18633 for that taxable year.\\n(II) A qualified investment partnership that is not required to file a federal return pursuant to Section 6031 of the Internal Revenue Code, relating to return of partnership income, shall file an information return as prescribed by the Franchise Tax Board for that taxable year.\\n(e) Notwithstanding anything in this section to the contrary, if the office of the Secretary of State files a certificate of cancellation pursuant to Section 17707.02 of the Corporations Code for any limited liability company, then paragraph (1) of subdivision (f) of Section 23153 shall apply to that limited liability company as if the limited liability company were properly treated as a corporation for that limited purpose only, and paragraph (2) of subdivision (f) of Section 23153 shall not apply. Nothing in this subdivision entitles a limited liability company to receive a reimbursement for any annual taxes or fees already paid.\\n(f) (1) Notwithstanding any provision of this section to the contrary, a limited liability company that is a small business solely owned by a deployed member of the United States Armed Forces shall not be subject to the tax imposed under this section for any taxable year the owner is deployed and the limited liability company operates at a loss or ceases operation.\\n(2) The Franchise Tax Board may promulgate regulations as necessary or appropriate to carry out the purposes of this subdivision, including a definition for “ceases operation.”\\n(3) For the purposes of this subdivision, all of the following definitions apply:\\n(A) “Deployed” means being called to active duty or active service during a period when a Presidential Executive order specifies that the United States is engaged in combat or homeland defense. “Deployed” does not include either of the following:\\n(i) Temporary duty for the sole purpose of training or processing.\\n(ii) A permanent change of station.\\n(B) “Operates at a loss” means a limited liability company’s expenses exceed its receipts.\\n(C) “Small business” means a limited liability company with total income from all sources derived from, or attributable, to the state of two hundred fifty thousand dollars ($250,000) or less.\\n(4) This subdivision shall become inoperative for taxable years beginning on or after January 1, 2018.\\nSEC. 2.\\nThis act provides for a tax levy within the meaning of Article IV of the Constitution and shall go into immediate effect.',\n",
       " 'summary': 'Existing\\nlaw,\\nlaw\\nimposes a minimum franchise tax of $800, except as provided, on every corporation incorporated in this state, qualified to transact intrastate business in this state, or doing business, as defined, in this state, and an annual tax in an amount equal to the minimum franchise tax on every limited liability company registered, qualified to transact business, or doing business in this state, as specified.\\nExisting law provides that certain corporations, the activities of which are limited to the receipt and disbursement of dividends and interest on securities, are not considered as doing business in this state.\\nExisting law requires every limited liability company subject to that annual tax to pay annually to this state a fee equal to specified amounts based upon total income from all sources attributable to this state. Existing law requires every partnership to file a return that includes specified information, verified by a written declaration made under the penalty of perjury and signed by one of the partners, within a specified time period.\\nThis bill, under those same circumstances related to the receipt and disbursement of dividends and interest on securities, would additionally provide that such a limited liability company is not considered as doing business in this state.\\nThis bill would exempt a limited liability company that is a qualified investment partnership, as defined, from that annual tax and fee by excluding it from the definition of a limited liability company. The bill would require that entity to submit a return under the conditions applicable to a partnership.\\nThis bill would take effect immediately as a tax levy.',\n",
       " 'title': 'An act to amend\\nSection 23102\\nSection\\n17941\\nof the Revenue and Taxation Code, relating to taxation, to take effect immediately, tax levy.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec775b1",
   "metadata": {},
   "source": [
    "There are two fields that you’ll want to use:\n",
    "\n",
    "text: the text of the bill which’ll be the input to the model.\n",
    "summary: a condensed version of text which’ll be the model target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef91a5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1a38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to load a T5 tokenizer to process text and summary:\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dba4e0",
   "metadata": {},
   "source": [
    "The preprocessing function you want to create needs to:\n",
    "\n",
    "1.Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "2.Use the keyword text_target argument when tokenizing labels.\n",
    "3.Truncate sequences to be no longer than the maximum length set by the max_length parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e9ef8",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use 🤗 Datasets map method. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb52448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b18d4f",
   "metadata": {},
   "source": [
    "Now create a batch of examples using DataCollatorForSeq2Seq. It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25dcfc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef157162",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9b3e6",
   "metadata": {},
   "source": [
    "Including a metric during training is often helpful for evaluating your model’s performance. You can quickly load a evaluation method with the 🤗 Evaluate library. For this task, load the ROUGE metric (see the 🤗 Evaluate quick tour to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ca98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c009730",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to compute to calculate the ROUGE metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017b2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb5655",
   "metadata": {},
   "source": [
    "Your compute_metrics function is ready to go now, and you’ll return to it when you setup your training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6c709",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56338c4f",
   "metadata": {},
   "source": [
    "You’re ready to start training your model now! Load T5 with AutoModelForSeq2SeqLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d481fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "122337a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56952fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\users\\vishal567795\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\datasets\\arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_billsum[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_billsum[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2e3c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99341a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921088fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishal567795\\Desktop\\Text_Summarizer\\my_awesome_billsum_model is already a clone of https://huggingface.co/vishal567795/my_awesome_billsum_model. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_billsum_model\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82b30346",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ebce045",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c332a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"summarize: Ram is a name that carries immense significance in various cultures, religions, and mythologies across the world. In Hinduism, Ram is considered one of the most revered and influential deities, known as the seventh avatar of Lord Vishnu. His story, beautifully depicted in the ancient Indian epic, the Ramayana, is a timeless tale that continues to inspire millions.Ram, also known as Rama or Shri Ram, is portrayed as an ideal human being, a paragon of virtue, and a true embodiment of righteousness. Born in Ayodhya to King Dasharatha and Queen Kaushalya, Ram was destined to fulfill a great purpose. As a young prince, he was adored by all, possessing a charming personality, unmatched skills, and an unwavering commitment to justice.However, Ram's life took a dramatic turn when his father was coerced into exiling him from the kingdom for fourteen years, due to the plotting of his stepmother, Kaikeyi. Despite the injustice he faced, Ram accepted his exile with humility and embraced the hardships that awaited him. Accompanied by his devoted wife, Sita, and loyal brother, Lakshmana, Ram embarked on an arduous journey through forests, facing numerous challenges and encountering mythical creatures.\n",
    "\n",
    "Throughout his exile, Ram demonstrated exceptional qualities of leadership, compassion, and unwavering devotion to duty. He valiantly protected sages, defeated formidable demons, and upheld righteousness in every situation. His unwavering commitment to truth and justice won him the unwavering loyalty of his followers and earned him the respect of divine beings.\n",
    "\n",
    "One of the most significant episodes in Ram's life is his encounter with the demon king Ravana. Ravana, with his ten heads and extraordinary powers, had abducted Sita, Ram's beloved wife. Driven by love and righteousness, Ram gathered an army of allies, including the brave monkey warrior Hanuman, to wage a fierce battle against Ravana and his forces. The epic confrontation culminated in Ram's triumph over evil, symbolizing the victory of good over darkness.\n",
    "\n",
    "Ram's unwavering love for Sita and his relentless pursuit to rescue her exemplify his devotion and commitment to his loved ones. He is seen as the ideal husband, respecting and cherishing his wife throughout their journey together. Their reunion after the arduous trials and tribulations is celebrated as an epitome of love, trust, and unwavering loyalty.\n",
    "\n",
    "The story of Ram has transcended religious boundaries and has become a universal symbol of morality, righteousness, and the triumph of good over evil. It teaches valuable lessons about the importance of adhering to one's duty, the power of unwavering faith, and the strength of familial and social bonds. Ram's virtuous character continues to inspire millions of people to strive for righteousness and embody the values he represents.\n",
    "\n",
    "In addition to his divine persona, Ram's legacy is also seen in the architectural marvels he left behind. Ayodhya, his birthplace, is revered as a holy site and is believed to be the location of his kingdom. The magnificent temples dedicated to Ram, such as the iconic Ram Janmabhoomi temple, attract devotees from all corners of the globe, fostering a sense of spirituality and devotion.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc58f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
      "c:\\users\\vishal567795\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Ram is considered one of the most revered and influential deities, known as the seventh avatar of Lord Vishnu. His story, beautifully depicted in the ancient Indian epic, the Ramayana, continues to inspire millions of people to strive for righteousness and embody the values he represents. Ram is portrayed as an ideal human being, a paragon of virtue, and a true embodiment of righteousness.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"stevhliu/my_awesome_billsum_model\")\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ada5e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"tf\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "467bff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'lm_head.weight', 'decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\",from_pt=True)\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b852d173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ramayana is a name that carries immense significance in various cultures, religions, and mythologies across the world. Ram is considered one of the most revered and influential deities, known as the seventh avatar of Lord Vishnu. His story, beautifully depicted in the ancient Indian epic, the Ramayana, is a timeless tale that continues to inspire millions. Ram is portrayed as an ideal human being, a paragon of virtue, and'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99610d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a945c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a098c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ad276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
